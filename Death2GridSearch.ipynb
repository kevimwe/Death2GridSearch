{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Death2GridSearch\n",
    "# Author: Kevin Okiah\n",
    "# 4/19/2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "from pandas.tools.plotting import table\n",
    "import  errno\n",
    "\n",
    "#import Classifiers \n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#import Regressors\n",
    "\n",
    "\n",
    "#Import model selection Utilities\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Import Evaluation metrics Classification\n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Import Evaluation metrics Regression\n",
    "from sklearn.metrics import explained_variance_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.metrics import median_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "#import Vizualization Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "from prettytable import PrettyTable\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read YAML file - File with the classification Algorithms and hyperparameters\n",
    "stream= open(\"setups/algos.yaml\", 'r')\n",
    "clf_dictionary = yaml.load(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#maps classfication objects to dict parms\n",
    "clf_mapper = {MLPClassifier: \"MLPClassifier\",\n",
    "              KNeighborsClassifier:\"KNeighborsClassifier\",\n",
    "              AdaBoostClassifier:\"AdaBoostClassifier\",\n",
    "              RandomForestClassifier:'RandomForestClassifier',\n",
    "              LogisticRegression:'LogisticRegression',\n",
    "              DecisionTreeClassifier: 'DecisionTreeClassifier'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LogisticRegression': {'penalty': ['l1', 'l2'],\n",
       "  'C': [0.1, 1, 10, 100],\n",
       "  'class_weight': ['balanced']},\n",
       " 'RandomForestClassifier': {'max_features': ['auto'],\n",
       "  'n_estimators': [10, 20, 100],\n",
       "  'min_samples_leaf': [2, 4, 10],\n",
       "  'max_depth': [3, 5, 10],\n",
       "  'criterion': ['gini', 'entropy']},\n",
       " 'DecisionTreeClassifier': {'criterion': ['gini', 'entropy'],\n",
       "  'min_samples_leaf': [2, 4],\n",
       "  'max_features': ['auto', 'sqrt', 'log2'],\n",
       "  'splitter': ['best', 'random'],\n",
       "  'min_samples_split': [2, 4]},\n",
       " 'MLPClassifier': {'hidden_layer_sizes': [100, 50, 500],\n",
       "  'activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
       "  'solver': ['lbfgs', 'sgd', 'adam']},\n",
       " 'KNeighborsClassifier': {'n_neighbors': [5, 10, 50, 100],\n",
       "  'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']},\n",
       " 'AdaBoostClassifier': {'n_estimators': [50, 100, 10],\n",
       "  'learning_rate': [1, 0.1, 0.01, 10]}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_dictionary['Classification']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading Dataset\n",
    "my_data = np.genfromtxt('data/numerai_data1.csv', delimiter=',',skip_header=1)\n",
    "dataX = pd.read_csv('data/heart.csv')\n",
    "\n",
    "\n",
    "def ModelingData(X, y, n_folds):\n",
    "    '''\n",
    "    Function to stitch Features(X), Response Variable(y) \n",
    "    and n_folds for modeling\n",
    "    X and y must ba arrays\n",
    "    '''\n",
    "    return (X, y, n_folds)\n",
    "\n",
    "def  GenerateXandY(df, y_name):\n",
    "    '''\n",
    "    Function to generate X and y arrays from pandas dataframe\n",
    "    \n",
    "    df = pandas Dataframe\n",
    "    y_name = Response varible or Target\n",
    "    \n",
    "    '''\n",
    "    y = np.array(df[y_name])\n",
    "    \n",
    "    X = np.array(df.loc[:, df.columns != y_name].values)\n",
    "    \n",
    "    return(X, y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(a_clf, data, clf_hyper={}):\n",
    "    '''\n",
    "    This function takes in a classification object, dataset, and clf parms\n",
    "    and performance metrics and runs\n",
    "    '''\n",
    "    M, L, n_folds = data # unpack data containter\n",
    "    kf = StratifiedKFold(n_splits=n_folds) # Establish the cross validation\n",
    "    ret = {} # classic explicaiton of results\n",
    "\n",
    "    for ids, (train_index, test_index) in enumerate(kf.split(M, L)):\n",
    "        clf = a_clf(**clf_hyper) # unpack paramters into clf is they exist\n",
    "        clf.fit(M[train_index], L[train_index])\n",
    "        pred = clf.predict(M[test_index])\n",
    "        ret[ids]= {'clf': clf,\n",
    "               'train_index': train_index,\n",
    "               'test_index': test_index,\n",
    "               'accuracy': accuracy_score(L[test_index], pred),\n",
    "               'precision': precision_score(L[test_index], pred),\n",
    "               'recall': recall_score(L[test_index], pred),\n",
    "               'f1_score': f1_score(L[test_index], pred),\n",
    "               'roc_auc_score':roc_auc_score(L[test_index], pred)}\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generate_Reports_Classification(Result, Algo, n, n_folds):\n",
    "    '''\n",
    "    Function generates reports for a given Classification Algorithm.\n",
    "    '''\n",
    "    \n",
    "    try:\n",
    "        temp_dir = \"results/ClassifiersRuns/\"\n",
    "        #temp_dir_fig = temp_dir+\"\\\\plots\"\n",
    "        os.makedirs(temp_dir)\n",
    "        #os.makedirs(temp_dir_fig)\n",
    "    except OSError as e:\n",
    "        if e.errno != errno.EEXIST:\n",
    "            raise\n",
    "    \n",
    "    folds =list(Result.keys())\n",
    "    clfs =[]\n",
    "    accuracy =[]\n",
    "    precision =[]\n",
    "    recall =[]\n",
    "    f1_score =[]\n",
    "    roc_auc_score =[]\n",
    "    \n",
    "    complete_name = os.path.join(temp_dir,Algo+\"_runs_details.txt\")\n",
    "\n",
    "\n",
    "    f= open(complete_name,\"a+\")\n",
    "    for i in folds:\n",
    "        temp = Result[i]\n",
    "        #print({temp['clf']})\n",
    "        clfs =clfs+[temp['clf']]\n",
    "        accuracy  = accuracy + [round(temp['accuracy'], 3)]\n",
    "        precision = precision + [round(temp['precision'], 3)]\n",
    "        recall= recall + [round(temp['recall'],3)]\n",
    "        f1_score = f1_score + [round(temp['f1_score'],3)]\n",
    "        roc_auc_score =roc_auc_score + [round(temp['roc_auc_score'],3)]\n",
    "    metrics = [accuracy, precision , recall, f1_score, roc_auc_score ]\n",
    "    metrics_names =['folds','accuracy', 'precision', 'recall', 'f1_score', 'roc_auc_score']\n",
    "    metrics_Names =['accuracy', 'precision', 'recall', 'f1_score', 'roc_auc_score']\n",
    "    metrics_avg = [np.mean(accuracy), np.mean(precision) , np.mean(recall), np.mean(f1_score), np.mean(roc_auc_score)]\n",
    "\n",
    "    metrics_avg = [ round(elem, 2) for elem in metrics_avg]\n",
    "\n",
    "    #row folds data\n",
    "    dat_dtype = {\n",
    "    'names' : (metrics_names),\n",
    "    'formats' : ('i', 'd', 'd', 'd', 'd', 'd')}\n",
    "    dat = np.zeros(n_folds, dat_dtype)\n",
    "\n",
    "    dat['folds'] = folds\n",
    "    dat['accuracy'] = accuracy\n",
    "    dat['precision'] = precision\n",
    "    dat['recall'] = recall\n",
    "    dat['f1_score'] = f1_score\n",
    "    dat['roc_auc_score'] = roc_auc_score\n",
    "\n",
    "    #averages\n",
    "    dat_dtype2 = {\n",
    "    'names' : (metrics_names),\n",
    "    'formats' : ('d', 'd', 'd', 'd', 'd', 'd')}\n",
    "    dat2 = np.zeros(1, dat_dtype2)\n",
    "\n",
    "    dat2['folds'] = n_folds\n",
    "    dat2['accuracy'] = metrics_avg[0]\n",
    "    dat2['precision'] = metrics_avg[1]\n",
    "    dat2['recall'] = metrics_avg[2]\n",
    "    dat2['f1_score'] = metrics_avg[3]\n",
    "    dat2['roc_auc_score'] = metrics_avg[4]\n",
    "    \n",
    "    f.write('-------------------------------------------------------------------' + '\\n')\n",
    "    f.write('Param Set ' + str(n) + '\\n')\n",
    "    f.write('-------------------------------------------------------------------' + '\\n')\n",
    "\n",
    "    x = PrettyTable(dat.dtype.names)\n",
    "    for row in dat:\n",
    "        x.add_row(row)\n",
    "\n",
    "    f.write(str(x))\n",
    "    f.write('\\n')\n",
    "\n",
    "    f.write('-------------------------------------------------------------------' + '\\n')\n",
    "    f.write('Average Scores for folds' + '\\n')\n",
    "    f.write('-------------------------------------------------------------------' + '\\n')\n",
    "\n",
    "    y = PrettyTable(dat2.dtype.names)\n",
    "    for row in dat2:\n",
    "        y.add_row(row)\n",
    "\n",
    "    f.write(str(y))\n",
    "\n",
    "    f.write('\\n')\n",
    "\n",
    "\n",
    "    f.write(str(clfs[0]) + '\\n')\n",
    "\n",
    "    f.close()\n",
    "\n",
    "    #Generate_Bars(folds, Algo,n, metrics, metrics_Names)\n",
    "    \n",
    "\n",
    "    return(folds, Algo, n, metrics, metrics_names, metrics_Names, metrics_avg, clfs[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SummaryReport(file):\n",
    "    \n",
    "    '''\n",
    "    Function to generate a summary report.\n",
    "    \n",
    "    '''\n",
    "\n",
    "    try:\n",
    "        temp_dir_fig = \"results/Summary\"\n",
    "        os.makedirs(temp_dir_fig)\n",
    "\n",
    "    except OSError as e:\n",
    "        if e.errno != errno.EEXIST:\n",
    "            raise\n",
    "    metrics_Names =['accuracy', 'precision', 'recall', 'f1_score', 'roc_auc_score']\n",
    "    temp =file['Summary'].keys()\n",
    "    Final_data = []\n",
    "    X_final ={}\n",
    "    Algos_List =list(temp)\n",
    "    for al in range(len(Algos_List)):\n",
    "        #keys = list(file['Summary'][Algos_List[al]].keys())\n",
    "        temp_data = []\n",
    "        summary_data = []\n",
    "        #Final_data = []\n",
    "        for i in range(5): # number of accruacy measures\n",
    "            temp_data = []\n",
    "            for k, v in file['Summary'][Algos_List[al]].items():\n",
    "                temp_data = temp_data+[(file['Summary'][Algos_List[al]][k][i])]\n",
    "            summary_data =summary_data+ [temp_data]\n",
    "        Final_data =Final_data+ [summary_data]\n",
    "\n",
    "    for met in range(len(metrics_Names)):\n",
    "        filename = str(metrics_Names[met])+\"_Summary.jpg\"\n",
    "        fig_name = os.path.join(temp_dir_fig,filename)\n",
    "        Met_score =[]\n",
    "        for i in Final_data:\n",
    "            Met_score = Met_score +[i[met]]\n",
    "        X = dict(zip(Algos_List, Met_score))\n",
    "        X_final.update({metrics_Names[met]:X})\n",
    "        plt.figure(figsize=(15,5))\n",
    "        plt.boxplot(X.values(), labels=X.keys())\n",
    "        #sns.boxplot(x=list(X.values()), y=list(X.keys()))\n",
    "        plt.title(metrics_Names[met], fontsize=18)\n",
    "        plt.xlabel('Algorithm', fontsize=12)\n",
    "        plt.ylabel(metrics_Names[met], fontsize=12)\n",
    "        plt.savefig(fig_name)\n",
    "\n",
    "    try:\n",
    "        temp_dir = \"results/Summary\"\n",
    "        #temp_dir_fig = temp_dir+\"\\\\plots\"\n",
    "        os.makedirs(temp_dir)\n",
    "        #os.makedirs(temp_dir_fig)\n",
    "    except OSError as e:\n",
    "        if e.errno != errno.EEXIST:\n",
    "            raise\n",
    "    bestscore =BestModel(file, X_final)\n",
    "    summary = SummaryTable(X_final)\n",
    "    summary_name = os.path.join(temp_dir,\"Summary.txt\")\n",
    "\n",
    "\n",
    "    f= open(summary_name,\"a+\")\n",
    "\n",
    "    f.write(\"------------------------------------------------------------------------------------\"+'\\n')\n",
    "    f.write(\"                                Performance Summary                                 \"+'\\n')\n",
    "    f.write(\"------------------------------------------------------------------------------------\"+'\\n')\n",
    "    f.write(\"                       Best performing model by perfomance Metrics                  \"+'\\n')\n",
    "    f.write(\"------------------------------------------------------------------------------------\"+'\\n')\n",
    "    f.write(str(bestscore))\n",
    "    f.write('\\n')\n",
    "    f.write('\\n')\n",
    "    f.write('\\n')\n",
    "\n",
    "    f.write(\"------------------------------------------------------------------------------------\"+'\\n')\n",
    "    f.write(\"                       Summary statistics by model                                  \"+'\\n')\n",
    "    f.write(\"------------------------------------------------------------------------------------\"+'\\n')\n",
    "    f.write(str(summary))\n",
    "    f.write(\"------------------------------------------------------------------------------------\"+'\\n')\n",
    "    f.write('\\n')\n",
    "\n",
    "\n",
    "    f.close()\n",
    "    #plt.show()\n",
    "    #print(\"Processing complete.....\")\n",
    "    return(X_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SummaryTable(dat):\n",
    "    '''\n",
    "    Generates summary table\n",
    "    '''\n",
    "\n",
    "    from prettytable import PrettyTable\n",
    "    title = ['accuracy', 'precision', 'recall', 'f1_score', 'roc_auc_score']\n",
    "    models = list(dat[title[0]].keys())\n",
    "\n",
    "    scores_final = {}\n",
    "    for met in title:\n",
    "        scores ={}\n",
    "        n =0\n",
    "        for i in list(dat[met].values()):\n",
    "            AVG = round(np.mean(i), 3)\n",
    "            MAX = round(np.max(i), 3)\n",
    "            MIN = round(np.min(i), 3)\n",
    "            StdDEV = round(np.std(i), 3)\n",
    "            temp = [AVG, MAX, MIN, StdDEV]\n",
    "            scores.update({models[n]:{' [AVG, MAX, MIN, StdDEV]':temp}})\n",
    "            n =n+1\n",
    "        #print(scores)\n",
    "\n",
    "        scores_final.update({met:scores})\n",
    "    x = PrettyTable()\n",
    "    x.field_names = [\"Measure\", \"Model\", \"AVG\", \"MAX\", \"MIN\", \"StdDEV\"]\n",
    "    for k , v in scores_final.items():\n",
    "        #print(k)\n",
    "        for k2, v2 in v.items():\n",
    "            #print(k2)\n",
    "            for k3, v3 in v2.items():\n",
    "                #print([k, k2, v3])\n",
    "                x.add_row([k, k2, v3[0], v3[1], v3[2], v3[3]])\n",
    "    return(x)\n",
    "\n",
    "def BestModel(file, dat):\n",
    "    '''\n",
    "    Function to return the best model and hyper params by different measures\n",
    "    '''\n",
    "    hypers = []\n",
    "    for key, val in file['clfs'].items():\n",
    "        for key1, val1 in val.items():\n",
    "            hypers = hypers+[val1]\n",
    "\n",
    "    models =[]\n",
    "    indexs = []\n",
    "    for key1, val1 in file['Summary'].items():\n",
    "        for key3, val3 in val1.items():\n",
    "                models =models +[key1]\n",
    "                indexs = indexs+ [key3]\n",
    "    scores_track = []\n",
    "\n",
    "    for k,j in dat.items():\n",
    "        temp = []\n",
    "        #print(k)\n",
    "        for k1, j1 in j.items():\n",
    "            temp = temp + j1\n",
    "        scores_track = scores_track+ [temp]\n",
    "\n",
    "    title = ['accuracy', 'precision', 'recall', 'f1_score', 'roc_auc_score']\n",
    "\n",
    "    x = PrettyTable()\n",
    "    x.field_names = [\"Measure\", \"Model\", \"BestScore\", \"Hyper Paramaters\"]\n",
    "\n",
    "    for i in range(len(title)):\n",
    "        tempX = [title[i],models[scores_track[i].index(max(scores_track[i]))],scores_track[i][scores_track[i].index(max(scores_track[i]))], hypers[scores_track[i].index(max(scores_track[i]))] ]\n",
    "        x.add_row(tempX)\n",
    "        #print(\"-----------------------------------\")\n",
    "    return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_max(data, color='yellow'):\n",
    "    '''\n",
    "    highlight the maximum in a Series or DataFrame\n",
    "    '''\n",
    "    attr = 'background-color: {}'.format(color)\n",
    "    if data.ndim == 1:  # Series from .apply(axis=0) or axis=1\n",
    "        is_max = data == data.max()\n",
    "        return [attr if v else '' for v in is_max]\n",
    "    else:  # from .apply(axis=None)\n",
    "        is_max = data == data.max().max()\n",
    "        return pd.DataFrame(np.where(is_max, attr, ''),\n",
    "                            index=data.index, columns=data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataX = pd.read_csv('data/heart.csv')\n",
    "\n",
    "def main(clf_dict = clf_dictionary, mapper = clf_mapper, data = dataX, Response ='target', n_folds = 10 , Analysis ='Classification'):\n",
    "    \n",
    "    '''\n",
    "    Function to put everything together\n",
    "    \n",
    "    clf_dict - Dictionary of Algorithms with hyperparamaters set from the yaml file\n",
    "    mapper= Maps clf dictionary to sklearn learn classifer objects\n",
    "    data = Cleaned pandas Dataframe ready for modeling\n",
    "    Resporse = this is the y\n",
    "    \n",
    "    '''   \n",
    "    #generate X and y from pandas df\n",
    "    X, y = GenerateXandY(data,'target')\n",
    "\n",
    "    #stitch X, y and n_foldes together\n",
    "    data =ModelingData(X, y, n_folds)\n",
    "\n",
    "    Metrics_Summary ={} #metrics summary\n",
    "    Clfs_Summary ={} #clfs Summary\n",
    "    Super_Dictionary ={} #super_disctionary\n",
    "\n",
    "    for key1 , value1 in clf_dict.items():\n",
    "        if key1 ==Analysis:\n",
    "            t =0# tracks param combination\n",
    "            for key2 , value2 in value1.items():\n",
    "                print(\"Running....\", key2)\n",
    "                clf = list(clf_mapper.keys())[list(clf_mapper.values()).index(key2)]\n",
    "                #print(clf)\n",
    "                Algo_avg = {}\n",
    "                Clfs_Set = {}\n",
    "                try:\n",
    "                    key3,value3 =zip(*value2.items())\n",
    "                    for values in product(*value3):\n",
    "                        hyperset =dict(zip(key3, values))\n",
    "                        #print(hyperset)\n",
    "                        result = run(clf, data, hyperset)\n",
    "                        folds_, Algo_, n_, metrics_, metrics_names_, metrics_Names_, metrics_avg_, clfs_= Generate_Reports_Classification(result, key2, t, n_folds) #generate reports and plots\n",
    "                        Clfs_Set.update({t:hyperset})\n",
    "                        Algo_avg.update({t:metrics_avg_})\n",
    "                        t = t + 1\n",
    "                except AttributeError:\n",
    "                    print(\"missing keys and values\")\n",
    "                Metrics_Summary.update({key2:Algo_avg})\n",
    "                Clfs_Summary.update({key2:Clfs_Set})\n",
    "                Super_Dictionary.update({\"Summary\":Metrics_Summary, \"clfs\":Clfs_Summary})\n",
    "        dat = SummaryReport(Super_Dictionary)\n",
    "        BestModel(Super_Dictionary, dat)\n",
    "        \n",
    "        datatable = dat.copy()\n",
    "        algo_list = list(datatable['accuracy'])\n",
    "        metrics =list(datatable.keys())\n",
    "        \n",
    "        results_temp =pd.DataFrame()\n",
    "        temp =[]\n",
    "        for j in metrics:\n",
    "            temp =[]\n",
    "            for i in algo_list:\n",
    "                #print(i, \":\",round(float(np.average(datatable[j][i])), 2))\n",
    "                temp = temp+ [round(float(np.max(datatable[j][i])), 2)]\n",
    "            results_temp[j] = temp\n",
    "            results_temp.index = algo_list\n",
    "\n",
    "        s = results_temp.style.apply(highlight_max)\n",
    "        print(\"processing Completed Successfully\")\n",
    "    return(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# run main \n",
    "#main(data =dataX, Response='target', n_folds=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
